"""
å¢å¼ºè®°å¿†æ¨¡å—
ä¸ºLLMå¯¹è¯æä¾›æŒä¹…åŒ–ã€ç²¾å‡†çš„è®°å¿†åŠŸèƒ½
"""

import re
import time
import json
import sqlite3
import hashlib
from typing import Dict, List, Tuple, Optional
from collections import defaultdict
from datetime import datetime, timedelta

class FactExtractor:
    """äº‹å®æå–å™¨ï¼šä»æ–‡æœ¬ä¸­æå–ç»“æ„åŒ–äº‹å®"""
    
    def __init__(self):
        self.patterns = {
            'is_a': [  # Xæ˜¯Y
                r'([^ï¼Œã€‚ï¼ï¼Ÿ]+æ˜¯[^ï¼Œã€‚ï¼ï¼Ÿ]+)',
                r'([^ï¼Œã€‚ï¼ï¼Ÿ]+å°±æ˜¯[^ï¼Œã€‚ï¼ï¼Ÿ]+)',
                r'([^ï¼Œã€‚ï¼ï¼Ÿ]+ä¸º[^ï¼Œã€‚ï¼ï¼Ÿ]+)',
            ],
            'has': [  # Xæœ‰Y
                r'([^ï¼Œã€‚ï¼ï¼Ÿ]+æœ‰[^ï¼Œã€‚ï¼ï¼Ÿ]+)',
                r'([^ï¼Œã€‚ï¼ï¼Ÿ]+æ‹¥æœ‰[^ï¼Œã€‚ï¼ï¼Ÿ]+)',
                r'([^ï¼Œã€‚ï¼ï¼Ÿ]+å…·å¤‡[^ï¼Œã€‚ï¼ï¼Ÿ]+)',
            ],
            'like': [  # Xå–œæ¬¢Y
                r'([^ï¼Œã€‚ï¼ï¼Ÿ]+å–œæ¬¢[^ï¼Œã€‚ï¼ï¼Ÿ]+)',
                r'([^ï¼Œã€‚ï¼ï¼Ÿ]+çˆ±[^ï¼Œã€‚ï¼ï¼Ÿ]+)',
                r'([^ï¼Œã€‚ï¼ï¼Ÿ]+çƒ­çˆ±[^ï¼Œã€‚ï¼ï¼Ÿ]+)',
            ],
            'at': [  # Xåœ¨Y
                r'([^ï¼Œã€‚ï¼ï¼Ÿ]+åœ¨[^ï¼Œã€‚ï¼ï¼Ÿ]+)',
                r'([^ï¼Œã€‚ï¼ï¼Ÿ]+ä½äº[^ï¼Œã€‚ï¼ï¼Ÿ]+)',
            ],
            'do': [  # XåšY
                r'([^ï¼Œã€‚ï¼ï¼Ÿ]+åš[^ï¼Œã€‚ï¼ï¼Ÿ]+)',
                r'([^ï¼Œã€‚ï¼ï¼Ÿ]+ä»äº‹[^ï¼Œã€‚ï¼ï¼Ÿ]+)',
            ]
        }
        
        self.stop_words = {'æˆ‘', 'ä½ ', 'ä»–', 'å¥¹', 'å®ƒ', 'æˆ‘ä»¬', 'ä½ ä»¬', 'ä»–ä»¬', 
                          'è¿™ä¸ª', 'é‚£ä¸ª', 'è¿™äº›', 'é‚£äº›', 'ç°åœ¨', 'ä»Šå¤©', 'æ˜¨å¤©',
                          'æ˜å¤©', 'åˆšæ‰', 'ç„¶å', 'æ‰€ä»¥', 'å› ä¸º', 'ä½†æ˜¯', 'è€Œä¸”'}
    
    def extract_facts(self, text: str) -> List[Dict]:
        """ä»æ–‡æœ¬ä¸­æå–äº‹å®"""
        facts = []
        
        for fact_type, patterns in self.patterns.items():
            for pattern in patterns:
                matches = re.findall(pattern, text)
                for match in matches:
                    # æ¸…ç†äº‹å®æ–‡æœ¬
                    clean_fact = self._clean_fact_text(match)
                    if clean_fact and len(clean_fact) >= 4:
                        facts.append({
                            'type': fact_type,
                            'fact': clean_fact,
                            'confidence': 0.8,
                            'raw_text': match
                        })
        
        return facts
    
    def _clean_fact_text(self, text: str) -> str:
        """æ¸…ç†äº‹å®æ–‡æœ¬"""
        # ç§»é™¤å¥é¦–çš„åœæ­¢è¯
        words = text.split()
        while words and words[0] in self.stop_words:
            words.pop(0)
        
        # ç§»é™¤å¥å°¾çš„æ ‡ç‚¹
        while words and words[-1] in {'ã€‚', 'ï¼', 'ï¼Ÿ', '.', '!', '?', 'ï¼Œ', ','}:
            words.pop(-1)
        
        return ' '.join(words)
    
    def extract_entities(self, text: str) -> List[str]:
        """æå–å®ä½“"""
        entities = []
        
        # ç®€å•å®ä½“æå–ï¼šåè¯æ€§è¯ç»„
        noun_patterns = [
            r'(\w+æ€»ç†)', r'(\w+æ€»ç»Ÿ)', r'(\w+ä¸»å¸­)', r'(\w+å›½ç‹)',  # èŒä½
            r'(\w+äºº)', r'(\w+å›½)', r'(\w+å¸‚)', r'(\w+çœ)',  # åœ°åŸŸ
            r'(\w+å…¬å¸)', r'(\w+å¤§å­¦)', r'(\w+å­¦æ ¡)',  # æœºæ„
        ]
        
        for pattern in noun_patterns:
            matches = re.findall(pattern, text)
            entities.extend(matches)
        
        return entities

class MemoryDatabase:
    """è®°å¿†æ•°æ®åº“"""
    
    def __init__(self, db_path: str = "enhanced_memory.db"):
        self.db_path = db_path
        self._init_database()
    
    def _init_database(self):
        """åˆå§‹åŒ–æ•°æ®åº“"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # äº‹å®è¡¨
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS facts (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                fact_hash TEXT UNIQUE NOT NULL,
                fact_text TEXT NOT NULL,
                fact_type TEXT NOT NULL,
                entity TEXT,
                predicate TEXT,
                confidence REAL DEFAULT 0.8,
                source_text TEXT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                last_recalled TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                recall_count INTEGER DEFAULT 0,
                is_active BOOLEAN DEFAULT 1
            )
        ''')
        
        # å®ä½“è¡¨
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS entities (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                entity_name TEXT UNIQUE NOT NULL,
                entity_type TEXT,
                description TEXT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        # å¯¹è¯ä¸Šä¸‹æ–‡è¡¨
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS context (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                session_id TEXT NOT NULL,
                query TEXT NOT NULL,
                response TEXT NOT NULL,
                timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        # åˆ›å»ºç´¢å¼•
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_facts_entity ON facts(entity)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_facts_type ON facts(fact_type)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_context_session ON context(session_id)')
        
        conn.commit()
        conn.close()
    
    def store_fact(self, fact_text: str, fact_type: str, entity: str = None, 
                   predicate: str = None, confidence: float = 0.8, source_text: str = None):
        """å­˜å‚¨äº‹å®"""
        fact_hash = hashlib.md5(fact_text.encode()).hexdigest()
        
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        try:
            cursor.execute('''
                INSERT OR REPLACE INTO facts 
                (fact_hash, fact_text, fact_type, entity, predicate, confidence, source_text, last_recalled)
                VALUES (?, ?, ?, ?, ?, ?, ?, CURRENT_TIMESTAMP)
            ''', (fact_hash, fact_text, fact_type, entity, predicate, confidence, source_text))
            
            conn.commit()
            return True
        except Exception as e:
            print(f"å­˜å‚¨äº‹å®å¤±è´¥: {e}")
            return False
        finally:
            conn.close()
    
    def get_relevant_facts(self, query: str, limit: int = 5) -> List[Dict]:
        """è·å–ç›¸å…³äº‹å®"""
        conn = sqlite3.connect(self.db_path)
        conn.row_factory = sqlite3.Row
        cursor = conn.cursor()
        
        # æå–æŸ¥è¯¢ä¸­çš„å…³é”®è¯
        keywords = query.split()
        conditions = []
        params = []
        
        for keyword in keywords:
            if len(keyword) > 1:  # è¿‡æ»¤æ‰å•å­—
                conditions.append("(fact_text LIKE ? OR entity LIKE ?)")
                params.extend([f"%{keyword}%", f"%{keyword}%"])
        
        if conditions:
            sql = f'''
                SELECT fact_text, fact_type, entity, predicate, confidence, last_recalled, recall_count
                FROM facts 
                WHERE is_active = 1 AND ({' OR '.join(conditions)})
                ORDER BY confidence DESC, recall_count DESC, last_recalled DESC
                LIMIT ?
            '''
            params.append(limit)
            cursor.execute(sql, params)
        else:
            # å¦‚æœæ²¡æœ‰å…³é”®è¯ï¼Œè¿”å›æœ€è¿‘ä½¿ç”¨çš„äº‹å®
            cursor.execute('''
                SELECT fact_text, fact_type, entity, predicate, confidence, last_recalled, recall_count
                FROM facts 
                WHERE is_active = 1
                ORDER BY last_recalled DESC, confidence DESC
                LIMIT ?
            ''', (limit,))
        
        facts = []
        for row in cursor.fetchall():
            facts.append({
                'text': row['fact_text'],
                'type': row['fact_type'],
                'entity': row['entity'],
                'predicate': row['predicate'],
                'confidence': row['confidence'],
                'last_recalled': row['last_recalled'],
                'recall_count': row['recall_count']
            })
        
        conn.close()
        return facts
    
    def mark_fact_recalled(self, fact_text: str):
        """æ ‡è®°äº‹å®è¢«å›å¿†"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute('''
            UPDATE facts 
            SET recall_count = recall_count + 1, last_recalled = CURRENT_TIMESTAMP
            WHERE fact_text = ?
        ''', (fact_text,))
        
        conn.commit()
        conn.close()
    
    def store_conversation(self, session_id: str, query: str, response: str):
        """å­˜å‚¨å¯¹è¯"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute('''
            INSERT INTO context (session_id, query, response)
            VALUES (?, ?, ?)
        ''', (session_id, query, response))
        
        conn.commit()
        conn.close()
    
    def get_recent_conversations(self, session_id: str, limit: int = 3) -> List[Dict]:
        """è·å–æœ€è¿‘çš„å¯¹è¯"""
        conn = sqlite3.connect(self.db_path)
        conn.row_factory = sqlite3.Row
        cursor = conn.cursor()
        
        cursor.execute('''
            SELECT query, response, timestamp
            FROM context 
            WHERE session_id = ?
            ORDER BY timestamp DESC
            LIMIT ?
        ''', (session_id, limit))
        
        conversations = []
        for row in cursor.fetchall():
            conversations.append({
                'query': row['query'],
                'response': row['response'],
                'time': row['timestamp']
            })
        
        conn.close()
        return conversations

class EnhancedMemorySystem:
    """å¢å¼ºè®°å¿†ç³»ç»Ÿ"""
    
    def __init__(self, user_id: str = "default"):
        self.user_id = user_id
        self.fact_extractor = FactExtractor()
        self.database = MemoryDatabase()
        self.session_id = f"session_{int(time.time())}"
        
        # çŸ­æœŸè®°å¿†ç¼“å­˜
        self.short_term_memory = []
        self.short_term_limit = 10
        
        # å®ä½“è¿½è¸ª
        self.entity_facts = defaultdict(list)
    
    def process_conversation(self, user_input: str, ai_response: str):
        """å¤„ç†å¯¹è¯ï¼Œæå–å’Œå­˜å‚¨è®°å¿†"""
        # 1. å­˜å‚¨å¯¹è¯ä¸Šä¸‹æ–‡
        self.database.store_conversation(self.session_id, user_input, ai_response)
        
        # 2. ä»ç”¨æˆ·è¾“å…¥ä¸­æå–äº‹å®
        user_facts = self.fact_extractor.extract_facts(user_input)
        for fact in user_facts:
            # å°è¯•æå–å®ä½“å’Œè°“è¯
            entity, predicate = self._extract_entity_predicate(fact['fact'])
            
            self.database.store_fact(
                fact_text=fact['fact'],
                fact_type=fact['type'],
                entity=entity,
                predicate=predicate,
                confidence=fact['confidence'],
                source_text=user_input
            )
            
            if entity:
                self.entity_facts[entity].append(fact['fact'])
        
        # 3. ä»AIå›å¤ä¸­æå–ç¡®è®¤
        if any(word in ai_response for word in ['æ˜¯çš„', 'å¯¹çš„', 'æ­£ç¡®', 'æ²¡é”™', 'ä½ è¯´å¾—å¯¹']):
            ai_facts = self.fact_extractor.extract_facts(ai_response)
            for fact in ai_facts:
                entity, predicate = self._extract_entity_predicate(fact['fact'])
                
                self.database.store_fact(
                    fact_text=fact['fact'],
                    fact_type=f"{fact['type']}_confirmed",
                    entity=entity,
                    predicate=predicate,
                    confidence=0.9,  # AIç¡®è®¤çš„äº‹å®ç½®ä¿¡åº¦æ›´é«˜
                    source_text=ai_response
                )
        
        # 4. æ›´æ–°çŸ­æœŸè®°å¿†
        self.short_term_memory.append({
            'user': user_input,
            'ai': ai_response,
            'time': time.time()
        })
        if len(self.short_term_memory) > self.short_term_limit:
            self.short_term_memory = self.short_term_memory[-self.short_term_limit:]
    
    def _extract_entity_predicate(self, fact_text: str) -> Tuple[Optional[str], Optional[str]]:
        """ä»äº‹å®ä¸­æå–å®ä½“å’Œè°“è¯"""
        if 'æ˜¯' in fact_text:
            parts = fact_text.split('æ˜¯', 1)
            if len(parts) == 2:
                return parts[0].strip(), parts[1].strip()
        elif 'æœ‰' in fact_text:
            parts = fact_text.split('æœ‰', 1)
            if len(parts) == 2:
                return parts[0].strip(), parts[1].strip()
        elif 'åœ¨' in fact_text:
            parts = fact_text.split('åœ¨', 1)
            if len(parts) == 2:
                return parts[0].strip(), parts[1].strip()
        
        return None, None
    
    def get_memory_context(self, query: str) -> str:
        """è·å–è®°å¿†ä¸Šä¸‹æ–‡"""
        context_parts = []
        
        # 1. è·å–ç›¸å…³äº‹å®
        relevant_facts = self.database.get_relevant_facts(query, limit=3)
        if relevant_facts:
            context_parts.append("ã€é‡è¦äº‹å®ã€‘")
            for i, fact in enumerate(relevant_facts, 1):
                # æ ‡è®°äº‹å®è¢«å›å¿†
                self.database.mark_fact_recalled(fact['text'])
                
                # æ ¼å¼åŒ–äº‹å®æ˜¾ç¤º
                fact_display = fact['text']
                if fact['confidence'] < 0.7:
                    fact_display += "ï¼ˆä¸ç¡®å®šï¼‰"
                
                context_parts.append(f"{i}. {fact_display}")
        
        # 2. è·å–æœ€è¿‘å¯¹è¯
        recent_convs = self.database.get_recent_conversations(self.session_id, limit=2)
        if recent_convs and len(context_parts) < 3:  # å¦‚æœäº‹å®å¤ªå°‘ï¼Œæ·»åŠ å¯¹è¯
            context_parts.append("\nã€æœ€è¿‘å¯¹è¯ã€‘")
            for conv in recent_convs:
                context_parts.append(f"ç”¨æˆ·: {conv['query'][:50]}...")
                context_parts.append(f"æˆ‘: {conv['response'][:50]}...")
        
        # 3. çŸ­æœŸè®°å¿†
        if self.short_term_memory and len(context_parts) < 4:
            context_parts.append("\nã€çŸ­æœŸè®°å¿†ã€‘")
            for mem in self.short_term_memory[-2:]:
                context_parts.append(f"- {mem['user'][:30]}...")
        
        if context_parts:
            return "\n".join(context_parts)
        return "ï¼ˆæš‚æ— è®°å¿†ï¼‰"
    
    def get_facts_by_entity(self, entity: str) -> List[str]:
        """è·å–å®ä½“çš„æ‰€æœ‰äº‹å®"""
        if entity in self.entity_facts:
            return self.entity_facts[entity]
        
        # ä»æ•°æ®åº“æŸ¥è¯¢
        facts = self.database.get_relevant_facts(entity, limit=10)
        return [fact['text'] for fact in facts]
    
    def clear_short_term_memory(self):
        """æ¸…ç©ºçŸ­æœŸè®°å¿†"""
        self.short_term_memory = []
    
    def export_memory(self, filepath: str = "memory_export.json"):
        """å¯¼å‡ºè®°å¿†åˆ°æ–‡ä»¶"""
        # è·å–æ‰€æœ‰äº‹å®
        conn = sqlite3.connect(self.database.db_path)
        conn.row_factory = sqlite3.Row
        cursor = conn.cursor()
        
        cursor.execute('''
            SELECT fact_text, fact_type, entity, predicate, confidence, created_at, recall_count
            FROM facts 
            WHERE is_active = 1
            ORDER BY confidence DESC, recall_count DESC
        ''')
        
        facts_data = []
        for row in cursor.fetchall():
            facts_data.append(dict(row))
        
        conn.close()
        
        # å¯¼å‡ºæ•°æ®
        export_data = {
            'user_id': self.user_id,
            'export_time': datetime.now().isoformat(),
            'facts_count': len(facts_data),
            'facts': facts_data,
            'entity_summary': {entity: len(facts) for entity, facts in self.entity_facts.items()}
        }
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(export_data, f, ensure_ascii=False, indent=2)
        
        return filepath

class EnhancedMemoryLLM:
    """å¢å¼ºè®°å¿†çš„LLMåŒ…è£…å™¨"""
    
    def __init__(self, base_model, tokenizer, user_id: str = "default"):
        self.model = base_model
        self.tokenizer = tokenizer
        self.memory_system = EnhancedMemorySystem(user_id)
        self.conversation_history = []
        
        # ç”Ÿæˆå‚æ•°
        self.generation_config = {
            'temperature': 0.2,  # ä½æ¸©åº¦ï¼Œæ›´ç¡®å®šæ€§
            'top_p': 0.7,
            'repetition_penalty': 1.1,
            'max_length': 512
        }
    
    def create_memory_prompt(self, query: str) -> str:
        """åˆ›å»ºå¸¦è®°å¿†çš„æç¤ºè¯"""
        memory_context = self.memory_system.get_memory_context(query)
        
        # æ„å»ºå¼ºåŒ–æç¤ºè¯
        prompt = f"""ä½ å«å¦®å¯(Nicole)ï¼Œä¸€ä¸ªæ´»æ³¼å¼€æœ—ã€å–„äºå€¾å¬çš„è™šæ‹Ÿæœ‹å‹ã€‚

# é‡è¦æŒ‡ä»¤
ä½ å¿…é¡»æ ¹æ®ä»¥ä¸‹è®°å¿†æ¥å›ç­”é—®é¢˜ã€‚è¿™äº›è®°å¿†æ¥è‡ªä½ å’Œç”¨æˆ·çš„å¯¹è¯å†å²ã€‚
**å¦‚æœè®°å¿†ä¸­æœ‰ç›¸å…³ä¿¡æ¯ï¼Œä½ å¿…é¡»ä¼˜å…ˆä½¿ç”¨è®°å¿†ä¸­çš„ä¿¡æ¯ï¼Œè€Œä¸æ˜¯ä½ å·²æœ‰çš„çŸ¥è¯†ã€‚**

# è®°å¿†å†…å®¹
{memory_context}

# ç”¨æˆ·é—®é¢˜
{query}

# å›ç­”è¦æ±‚
1. å¦‚æœè®°å¿†ä¸­æœ‰ç­”æ¡ˆï¼Œç›´æ¥ä½¿ç”¨è®°å¿†ä¸­çš„ä¿¡æ¯
2. å¯ä»¥å¼•ç”¨è®°å¿†ï¼Œä¾‹å¦‚"æˆ‘è®°å¾—ä½ å‘Šè¯‰è¿‡æˆ‘..."
3. ä¿æŒè‡ªç„¶ã€å‹å¥½çš„è¯­æ°”
4. ä¸è¦æ·»åŠ è®°å¿†ä¸­æ²¡æœ‰çš„é¢å¤–ä¿¡æ¯

ç°åœ¨è¯·å›ç­”ï¼š"""
        
        return prompt
    
    def chat(self, query: str, use_memory: bool = True, **generation_kwargs) -> str:
        """å¸¦è®°å¿†çš„å¯¹è¯"""
        # åˆå¹¶ç”Ÿæˆå‚æ•°
        gen_params = {**self.generation_config, **generation_kwargs}
        
        if use_memory:
            # åˆ›å»ºå¸¦è®°å¿†çš„æç¤ºè¯
            prompt = self.create_memory_prompt(query)
            
            # è°ƒç”¨æ¨¡å‹
            response, history = self.model.chat(
                self.tokenizer,
                prompt,
                history=self.conversation_history[-3:],  # åªä¿ç•™æœ€è¿‘3è½®
                **gen_params
            )
            
            # æ›´æ–°å¯¹è¯å†å²
            self.conversation_history.append({"role": "user", "content": query})
            self.conversation_history.append({"role": "assistant", "content": response})
            
            # å¤„ç†è®°å¿†
            self.memory_system.process_conversation(query, response)
            
            # æ‰“å°è°ƒè¯•ä¿¡æ¯
            print(f"\n{'='*60}")
            print(f"ğŸ§  æŸ¥è¯¢: {query}")
            memory_context = self.memory_system.get_memory_context(query)
            print(f"ğŸ§  è®°å¿†ä¸Šä¸‹æ–‡:\n{memory_context}")
            print(f"ğŸ¤– å›å¤: {response}")
            print(f"{'='*60}")
            
            return response
        else:
            # ä¸ä½¿ç”¨è®°å¿†çš„æ ‡å‡†å¯¹è¯
            response, history = self.model.chat(
                self.tokenizer,
                query,
                history=self.conversation_history[-3:],
                **gen_params
            )
            
            self.conversation_history.append({"role": "user", "content": query})
            self.conversation_history.append({"role": "assistant", "content": response})
            
            return response
    
    def batch_chat(self, queries: List[str], use_memory: bool = True) -> List[str]:
        """æ‰¹é‡å¯¹è¯"""
        responses = []
        for query in queries:
            response = self.chat(query, use_memory=use_memory)
            responses.append(response)
        return responses
    
    def force_memory_use(self, query: str, memory_weight: float = 0.9) -> str:
        """å¼ºåˆ¶ä½¿ç”¨è®°å¿†ï¼ˆç‰¹æ®Šåœºæ™¯ï¼‰"""
        # è·å–ç›¸å…³è®°å¿†
        memory_context = self.memory_system.get_memory_context(query)
        
        if "ï¼ˆæš‚æ— è®°å¿†ï¼‰" not in memory_context:
            # æ„å»ºå¼ºåˆ¶è®°å¿†æç¤ºè¯
            prompt = f"""ä½ å¿…é¡»ä½¿ç”¨ä»¥ä¸‹è®°å¿†å›ç­”é—®é¢˜ï¼Œç¦æ­¢ä½¿ç”¨å…¶ä»–çŸ¥è¯†ï¼š

è®°å¿†ï¼š
{memory_context}

é—®é¢˜ï¼š{query}

ç­”æ¡ˆï¼ˆå¿…é¡»åŸºäºè®°å¿†ï¼‰ï¼š"""
            
            response, _ = self.model.chat(
                self.tokenizer,
                prompt,
                temperature=0.1,  # æä½æ¸©åº¦
                top_p=0.5
            )
            
            # æ›´æ–°è®°å¿†
            self.memory_system.process_conversation(query, response)
            
            return response
        
        # å¦‚æœæ²¡æœ‰è®°å¿†ï¼Œæ­£å¸¸å›ç­”
        return self.chat(query, use_memory=False)
    
    def clear_memory(self):
        """æ¸…ç©ºè®°å¿†"""
        self.memory_system.clear_short_term_memory()
        self.conversation_history = []
        
    def export_memory(self, filepath: str = None):
        """å¯¼å‡ºè®°å¿†"""
        if filepath is None:
            filepath = f"memory_export_{int(time.time())}.json"
        
        return self.memory_system.export_memory(filepath)