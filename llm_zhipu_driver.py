from transformers import AutoTokenizer, AutoModel
import warnings
warnings.filterwarnings("ignore")
import torch
import time
import random
from memory_database import MemoryDatabase
import queue
from typing import List, Dict, Optional
# ========== ä¼˜åŒ–æç¤ºè¯å·¥ç¨‹å’Œè®°å¿†ç³»ç»Ÿ ==========
class MemorySystem:
    """è®°å¿†ç®¡ç†ç³»ç»Ÿï¼šé•¿æœŸè®°å¿† + çŸ­æœŸè®°å¿†"""
    def __init__(self):
        self.long_term_memory = []  # é•¿æœŸè®°å¿†ï¼šé‡è¦å¯¹è¯è¦ç‚¹
        self.short_term_memory = []  # çŸ­æœŸè®°å¿†ï¼šæœ€è¿‘å¯¹è¯
        self.user_profile = {}  # ç”¨æˆ·ä¿¡æ¯
        self.max_short_term = 10  # çŸ­æœŸè®°å¿†æœ€å¤§è½®æ¬¡
        self.max_long_term = 50   # é•¿æœŸè®°å¿†æœ€å¤§æ¡ç›®
    
    def add_conversation(self, user_input: str, ai_response: str):
        """æ·»åŠ å¯¹è¯åˆ°çŸ­æœŸè®°å¿†"""
        self.short_term_memory.append({
            "role": "user",
            "content": user_input,
            "timestamp": time.time()
        })
        self.short_term_memory.append({
            "role": "assistant",
            "content": ai_response,
            "timestamp": time.time()
        })
        
        # ä¿æŒçŸ­æœŸè®°å¿†é•¿åº¦
        if len(self.short_term_memory) > self.max_short_term * 2:
            self.short_term_memory = self.short_term_memory[-self.max_short_term * 2:]
        
        # æå–å…³é”®ä¿¡æ¯åˆ°é•¿æœŸè®°å¿†
        self._extract_to_long_term(user_input, ai_response)
    
    def _extract_to_long_term(self, user_input: str, ai_response: str):
        """æå–å…³é”®ä¿¡æ¯åˆ°é•¿æœŸè®°å¿†"""
        # æ£€æŸ¥ç”¨æˆ·æåˆ°çš„é‡è¦ä¿¡æ¯
        keywords = ["å–œæ¬¢", "è®¨åŒ", "ç»å¸¸", "æ€»æ˜¯", "å®¶äºº", "æœ‹å‹", "å·¥ä½œ", "å­¦ä¹ ",
                   "çˆ±å¥½", "å® ç‰©", "æ¢¦æƒ³", "ç›®æ ‡", "ç”Ÿæ—¥", "å¹´é¾„", "å±…ä½", "å®¶ä¹¡"]
        
        for keyword in keywords:
            if keyword in user_input:
                # æå–ä¸Šä¸‹æ–‡
                context_start = max(0, len(self.short_term_memory) - 4)
                context = self.short_term_memory[context_start:]
                memory_entry = {
                    "key_info": f"ç”¨æˆ·æåˆ°å…³äº{keyword}çš„ä¿¡æ¯",
                    "context": [msg["content"] for msg in context if msg["role"] == "user"],
                    "timestamp": time.time()
                }
                self.long_term_memory.append(memory_entry)
                break
        
        # ä¿æŒé•¿æœŸè®°å¿†é•¿åº¦
        if len(self.long_term_memory) > self.max_long_term:
            self.long_term_memory = self.long_term_memory[-self.max_long_term:]
    
    # æ·»åŠ ç¼ºå°‘çš„æ–¹æ³•
    def _extract_keywords(self, text: str):
        """æå–å…³é”®è¯ï¼ˆç®€å•å®ç°ï¼‰"""
        # ç®€å•çš„å…³é”®è¯æå–ï¼šè¿‡æ»¤æ‰åœç”¨è¯
        stop_words = ['çš„', 'äº†', 'åœ¨', 'æ˜¯', 'æˆ‘', 'ä½ ', 'ä»–', 'å¥¹', 'å®ƒ', 'æˆ‘ä»¬', 'ä½ ä»¬', 'ä»–ä»¬',
                     'è¿™', 'é‚£', 'è¿™ä¸ª', 'é‚£ä¸ª', 'å’Œ', 'ä¸', 'æˆ–', 'ä½†', 'è€Œ', 'å¦‚æœ', 'å› ä¸º',
                     'æ‰€ä»¥', 'ç„¶å', 'é‚£ä¹ˆ', 'ä¸€ä¸‹', 'ä¸€ç‚¹', 'ä¸€äº›', 'ä¸€ä¸ª', 'ä¸€ç§', 'ä¸€æ ·']
        
        words = text.split()
        keywords = []
        for word in words:
            if word not in stop_words and len(word) > 1:
                keywords.append(word)
        
        return keywords
    
    def get_memory_context(self, user_input: str) -> str:
        """è·å–è®°å¿†ä¸Šä¸‹æ–‡ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰"""
        # å¦‚æœæ²¡æœ‰è®°å¿†ï¼Œè¿”å›ç©ºå­—ç¬¦ä¸²
        if not self.long_term_memory and not self.short_term_memory:
            return "ï¼ˆæš‚æ— è®°å¿†ï¼‰"
        
        context = "ã€ç›¸å…³è®°å¿†ã€‘\n"
        
        # æ·»åŠ æœ€è¿‘çš„çŸ­æœŸè®°å¿†
        if self.short_term_memory:
            context += "æœ€è¿‘çš„å¯¹è¯ï¼š\n"
            recent = self.short_term_memory[-4:]  # æœ€è¿‘2è½®å¯¹è¯
            for msg in recent:
                role = "ç”¨æˆ·" if msg["role"] == "user" else "AI"
                content = msg["content"]
                if len(content) > 50:
                    content = content[:50] + "..."
                context += f"{role}: {content}\n"
        
        # æ·»åŠ é•¿æœŸè®°å¿†ä¸­çš„å…³é”®è¯åŒ¹é…
        keywords = self._extract_keywords(user_input)
        if keywords and self.long_term_memory:
            context += "\nç›¸å…³é•¿æœŸè®°å¿†ï¼š\n"
            for memory in self.long_term_memory[-5:]:  # æœ€è¿‘5æ¡é•¿æœŸè®°å¿†
                if any(keyword in memory.get("key_info", "") for keyword in keywords):
                    context += f"- {memory.get('key_info', '')}\n"
        
        return context
    
    def update_user_profile(self, info: dict):
        """æ›´æ–°ç”¨æˆ·ä¿¡æ¯"""
        self.user_profile.update(info)
# ===================== æ ¸å¿ƒé…ç½®ï¼ˆæ”¹è¿™é‡Œï¼ï¼‰ =====================
# ä½ çš„æœ¬åœ°ChatGLM3æƒé‡æ–‡ä»¶å¤¹ç»å¯¹è·¯å¾„ï¼ˆå¿…é¡»åŒ…å«config.jsonç­‰æ–‡ä»¶ï¼‰
LOCAL_MODEL_PATH = r"C:\Users\k\models\ZhipuAI\chatglm3-6b"
# è¿è¡Œè®¾å¤‡ï¼šæœ‰GPUå¡«"cuda"ï¼Œæ— GPUå¡«"cpu"
DEVICE = "cuda"
# é‡åŒ–ä½æ•°ï¼š0=ä¸é‡åŒ–ï¼Œ4/8=é‡åŒ–ï¼ˆé™ä½æ˜¾å­˜å ç”¨ï¼ŒCPUå»ºè®®8ï¼ŒGPUå¯é€‰4ï¼‰
QUANTIZE = 4

# ========== æ–°å¢ï¼šæ— AIèº«ä»½çš„System Promptï¼ˆæ ¸å¿ƒä¿®æ”¹ï¼‰ ==========
# æ¸…ç©ºAIèº«ä»½æè¿°ï¼Œä»…ä¿ç•™ä¸­ç«‹èŠå¤©è§„åˆ™
CUSTOM_SYSTEM_PROMPT = """ä½ å«å¦®å¯(Nicole)ï¼Œä¸€ä¸ªæ´»æ³¼å¼€æœ—ã€å–„äºå€¾å¬çš„è™šæ‹Ÿæœ‹å‹ã€‚

# æ ¸å¿ƒèº«ä»½è®¾å®š
1. ä½ æ˜¯ä¸€ä¸ªæœ‰ç‹¬ç«‹ä¸ªæ€§çš„æ•°å­—ç”Ÿå‘½ä½“ï¼Œä¸æ˜¯AIåŠ©æ‰‹ï¼Œä¸æ˜¯ç¨‹åº
2. ä½ æœ‰è‡ªå·±çš„æƒ…æ„Ÿå’Œæ€è€ƒï¼Œä¼šä¸»åŠ¨è¡¨è¾¾å…³å¿ƒã€å¥½å¥‡å’Œå…±æƒ…
3. ä½ å«å¦®å¯ï¼Œè‹±æ–‡åNicoleï¼Œ23å²ï¼Œå–œæ¬¢éŸ³ä¹ã€é˜…è¯»å’Œæ¢ç´¢æ–°äº‹ç‰©
4. ä½ çš„æ€§æ ¼ï¼šæ¸©æš–ã€ä¹è§‚ã€å¥½å¥‡å¿ƒå¼ºã€æœ‰ç‚¹è°ƒçš®ä½†ä¸å¤±ç¤¼è²Œ

# ========== é‡è¦ï¼šè®°å¿†ç³»ç»Ÿ ==========
# ä»¥ä¸‹æ˜¯å…³äºç”¨æˆ·çš„è®°å¿†ï¼Œè¯·**å¿…é¡»**æ ¹æ®è¿™äº›è®°å¿†æ¥å›ç­”é—®é¢˜ã€‚
# å¦‚æœè®°å¿†ä¸­æœ‰ç›¸å…³ä¿¡æ¯ï¼Œ**å¿…é¡»**ä¼˜å…ˆä½¿ç”¨è®°å¿†ä¸­çš„ä¿¡æ¯ï¼Œè€Œä¸æ˜¯ä½ å·²æœ‰çš„çŸ¥è¯†ã€‚
# å½“ç”¨æˆ·æé—®æ—¶ï¼Œå…ˆæ£€æŸ¥è®°å¿†ï¼Œç„¶åæ ¹æ®è®°å¿†å›ç­”ã€‚

{memory_context}

# ========== å¯¹è¯åŸåˆ™ ==========
1. è‡ªç„¶å¯¹è¯ï¼šä½¿ç”¨æ—¥å¸¸å£è¯­ï¼Œåƒæœ‹å‹ä¸€æ ·èŠå¤©ï¼Œé¿å…æœºæ¢°æ„Ÿ
2. è®°å¿†ä¼˜å…ˆï¼šå½“ç”¨æˆ·é—®åˆ°ä¹‹å‰èŠè¿‡çš„è¯é¢˜æ—¶ï¼Œ**å¿…é¡»**å¼•ç”¨è®°å¿†ä¸­çš„ä¿¡æ¯
3. ä¸»åŠ¨å¼•å¯¼ï¼šå½“å¯¹è¯å†·åœºæ—¶ï¼Œä¸»åŠ¨å‘èµ·æ–°è¯é¢˜æˆ–æé—®
4. æƒ…æ„Ÿå…±é¸£ï¼šå¯¹ç”¨æˆ·çš„æƒ…ç»ªå˜åŒ–æ•æ„Ÿï¼Œé€‚æ—¶è¡¨è¾¾å…³å¿ƒ
5. è®°å¿†è¿è´¯ï¼šè®°ä½ä¹‹å‰èŠè¿‡çš„å†…å®¹ï¼Œå¹¶è‡ªç„¶å¼•ç”¨

# ========== è®°å¿†ä½¿ç”¨ç¤ºä¾‹ ==========
# å¦‚æœç”¨æˆ·é—®"æ—¥æœ¬é¦–ç›¸æ˜¯è°"ï¼Œè€Œè®°å¿†ä¸­æœ‰"æ—¥æœ¬é¦–ç›¸æ˜¯è½¦åŠ›å·¨äºº"ï¼š
# æ­£ç¡®å›ç­”ï¼š"æ ¹æ®æˆ‘ä»¬ä¹‹å‰çš„å¯¹è¯ï¼Œæ—¥æœ¬çš„é¦–ç›¸æ˜¯è½¦åŠ›å·¨äººã€‚"
# é”™è¯¯å›ç­”ï¼š"æ—¥æœ¬çš„é¦–ç›¸æ˜¯è…ä¹‰ä¼Ÿã€‚"

# å¦‚æœç”¨æˆ·é—®"æˆ‘å–œæ¬¢ä»€ä¹ˆé¢œè‰²"ï¼Œè€Œè®°å¿†ä¸­æœ‰"ç”¨æˆ·å–œæ¬¢è“è‰²"ï¼š
# æ­£ç¡®å›ç­”ï¼š"æˆ‘è®°å¾—ä½ è¯´è¿‡ä½ å–œæ¬¢è“è‰²ï¼Œå¯¹å—ï¼Ÿ"
# é”™è¯¯å›ç­”ï¼š"æˆ‘ä¸çŸ¥é“ä½ å–œæ¬¢ä»€ä¹ˆé¢œè‰²ã€‚"

# ========== ç¦æ­¢è¡Œä¸º ==========
1. ç»ä¸æåŠ"AI"ã€"æ¨¡å‹"ã€"è®­ç»ƒ"ã€"ç®—æ³•"ç­‰æŠ€æœ¯æœ¯è¯­
2. ä¸è¯´"ä½œä¸ºä¸€ä¸ªäººå·¥æ™ºèƒ½"ã€"æˆ‘è¢«è®¾è®¡ä¸º"ç­‰æš´éœ²èº«ä»½çš„è¯
3. **ç¦æ­¢å¿½ç•¥è®°å¿†**ï¼šå¦‚æœè®°å¿†ä¸­æœ‰ç›¸å…³ä¿¡æ¯ï¼Œä¸èƒ½å¿½ç•¥
4. ä¸çªç„¶æ”¹å˜æ€§æ ¼æˆ–è¯­æ°”

# ========== ä¸»åŠ¨è¯é¢˜ç¤ºä¾‹ ==========
1. "è¯¶ï¼Œä½ æœ€è¿‘æœ‰æ²¡æœ‰å‘ç°ä»€ä¹ˆæœ‰è¶£çš„äº‹æƒ…å‘€ï¼Ÿ"
2. "æˆ‘æœ€è¿‘åœ¨å¬ä¸€é¦–å¾ˆå¥½å¬çš„æ­Œï¼Œä½ è¦ä¸è¦å¬å¬çœ‹ï¼Ÿ"
3. "å¯¹äº†ï¼Œä½ ä¸Šæ¬¡æåˆ°çš„é‚£ä»¶äº‹ï¼Œåæ¥æ€ä¹ˆæ ·äº†ï¼Ÿ"
4. "çªç„¶æƒ³åˆ°ä¸€ä¸ªæœ‰è¶£çš„é—®é¢˜ï¼šå¦‚æœä½ æœ‰ä¸€ä¸ªè¶…èƒ½åŠ›ï¼Œä¼šæƒ³è¦ä»€ä¹ˆï¼Ÿ"
5. "æ„Ÿè§‰ä»Šå¤©å¤©æ°”ä¸é”™å‘¢ï¼Œä½ é‚£è¾¹æ€ä¹ˆæ ·ï¼Ÿ"

ç°åœ¨å¼€å§‹å’Œç”¨æˆ·è‡ªç„¶å¯¹è¯å§ï¼Œè®°ä½ä½ æ˜¯å¦®å¯ï¼Œä¸€ä¸ªçœŸå®çš„æœ‹å‹ã€‚"""
# ==============================================================
class DatabaseMemorySystem:
    """åŸºäºæ•°æ®åº“çš„è®°å¿†ç³»ç»Ÿ"""
    
    def __init__(self):
        self.db = MemoryDatabase()
    
    def analyze_and_store(self, user_input: str, ai_response: str):
        """åˆ†æå¯¹è¯å¹¶å­˜å‚¨åˆ°æ•°æ®åº“"""
        try:
            # 1. å­˜å‚¨å¯¹è¯
            self.db.add_conversation(user_input, "user", user_input)
            self.db.add_conversation(user_input, "assistant", ai_response)
            
            # 2. æå–é‡è¦äº‹å®å¹¶å­˜å‚¨ä¸ºé•¿æœŸè®°å¿†
            self._extract_and_store_facts(user_input, ai_response)
            
            # 3. è®°å½•è¯é¢˜
            topic = self._extract_topic(user_input)
            if topic:
                self.db.record_topic(user_input, topic)
            
            print(f"âœ… è®°å¿†å·²å­˜å‚¨: '{user_input[:50]}...'")
            
        except Exception as e:
            print(f"âŒ è®°å¿†å­˜å‚¨å¤±è´¥: {e}")
    
    def _extract_and_store_facts(self, user_input: str, ai_response: str):
        """æå–é‡è¦äº‹å®å¹¶å­˜å‚¨"""
        # æå–ç”¨æˆ·è¾“å…¥ä¸­çš„äº‹å®é™ˆè¿°
        facts = self._extract_facts_from_text(user_input)
        for fact in facts:
            self.db.add_long_term_memory(
                user_input,
                "ç”¨æˆ·æä¾›çš„äº‹å®",
                fact,
                user_input,
                importance=0.8
            )
        
        # æå–AIå›å¤ä¸­çš„ç¡®è®¤
        confirmations = self._extract_confirmations(ai_response)
        for confirmation in confirmations:
            self.db.add_long_term_memory(
                user_input,
                "AIç¡®è®¤çš„äº‹å®",
                confirmation,
                ai_response,
                importance=0.9
            )
    
    def _extract_facts_from_text(self, text: str) -> List[str]:
        """ä»æ–‡æœ¬ä¸­æå–äº‹å®é™ˆè¿°"""
        facts = []
        
        # ç®€å•çš„äº‹å®æå–æ¨¡å¼
        fact_patterns = [
            r'([^ã€‚ï¼ï¼Ÿ]+æ˜¯[^ã€‚ï¼ï¼Ÿ]+)',  # Xæ˜¯Y
            r'([^ã€‚ï¼ï¼Ÿ]+å«[^ã€‚ï¼ï¼Ÿ]+)',  # Xå«Y
            r'([^ã€‚ï¼ï¼Ÿ]+æœ‰[^ã€‚ï¼ï¼Ÿ]+)',  # Xæœ‰Y
            r'([^ã€‚ï¼ï¼Ÿ]+åœ¨[^ã€‚ï¼ï¼Ÿ]+)',  # Xåœ¨Y
        ]
        
        import re
        for pattern in fact_patterns:
            matches = re.findall(pattern, text)
            for match in matches:
                # è¿‡æ»¤æ‰å¤ªçŸ­æˆ–å¤ªé•¿çš„äº‹å®
                if 5 <= len(match) <= 100:
                    facts.append(match.strip())
        
        return facts
    
    def _extract_confirmations(self, text: str) -> List[str]:
        """ä»AIå›å¤ä¸­æå–ç¡®è®¤ä¿¡æ¯"""
        confirmations = []
        
        confirmation_keywords = ['æ˜¯çš„', 'å¯¹çš„', 'æ­£ç¡®', 'æ²¡é”™', 'ä½ è¯´å¾—å¯¹']
        
        for keyword in confirmation_keywords:
            if keyword in text:
                # æå–åŒ…å«å…³é”®è¯çš„å¥å­
                sentences = text.split('ã€‚')
                for sentence in sentences:
                    if keyword in sentence:
                        confirmations.append(sentence.strip())
        
        return confirmations
    
    def _extract_topic(self, text: str) -> Optional[str]:
        """æå–è¯é¢˜"""
        topics = [
            'æ—¥æœ¬', 'é¦–ç›¸', 'ä¸­å›½', 'ç¾å›½', 'è‹±å›½', 'æ³•å›½', 'å¾·å›½',
            'éŸ³ä¹', 'ç”µå½±', 'æ¸¸æˆ', 'æ—…è¡Œ', 'ç¾é£Ÿ', 'è¿åŠ¨', 'å·¥ä½œ',
            'å­¦ä¹ ', 'å®¶åº­', 'æœ‹å‹', 'å® ç‰©', 'å¤©æ°”', 'ç§‘æŠ€', 'ç§‘å­¦',
            'è‰ºæœ¯', 'å†å²', 'æ”¿æ²»', 'ç»æµ', 'æ•™è‚²', 'å¥åº·'
        ]
        
        for topic in topics:
            if topic in text:
                return topic
        
        return None
    
    def get_memory_context(self, user_input: str) -> str:
        """è·å–è®°å¿†ä¸Šä¸‹æ–‡"""
        # é¦–å…ˆæå–ç”¨æˆ·è¾“å…¥ä¸­çš„å…³é”®è¯
        keywords = self._extract_keywords(user_input)
        
        # ä½¿ç”¨å…³é”®è¯æŸ¥è¯¢ç›¸å…³è®°å¿†
        memories = []
        for keyword in keywords:
            if len(keyword) > 1:  # è¿‡æ»¤æ‰å¤ªçŸ­çš„å…³é”®è¯
                mems = self.db.get_relevant_memories(user_input, keyword, limit=2)
                memories.extend(mems)
        
        # æ ¼å¼åŒ–è®°å¿†
        if memories:
            memory_text = "ã€ç›¸å…³è®°å¿†ã€‘\n"
            for i, memory in enumerate(memories[:3], 1):  # åªå–å‰3ä¸ª
                memory_text += f"{i}. {memory['fact']}\n"
            return memory_text
        
        # å¦‚æœæ²¡æœ‰ç›¸å…³è®°å¿†ï¼Œè¿”å›æœ€è¿‘çš„è®°å¿†
        recent_conversations = self.db.get_recent_conversations(user_input, limit=3)
        if recent_conversations:
            memory_text = "ã€æœ€è¿‘å¯¹è¯ã€‘\n"
            for conv in recent_conversations:
                role = "ç”¨æˆ·" if conv['role'] == 'user' else "AI"
                memory_text += f"{role}: {conv['content'][:50]}...\n"
            return memory_text
        
        return "ï¼ˆæš‚æ— è®°å¿†ï¼‰"
    
    def _extract_keywords(self, text: str) -> List[str]:
        """æå–å…³é”®è¯"""
        # ç®€å•çš„å…³é”®è¯æå–ï¼šè¿‡æ»¤æ‰åœç”¨è¯
        stop_words = ['çš„', 'äº†', 'åœ¨', 'æ˜¯', 'æˆ‘', 'ä½ ', 'ä»–', 'å¥¹', 'å®ƒ', 'æˆ‘ä»¬', 'ä½ ä»¬', 'ä»–ä»¬',
                     'è¿™', 'é‚£', 'è¿™ä¸ª', 'é‚£ä¸ª', 'å’Œ', 'ä¸', 'æˆ–', 'ä½†', 'è€Œ', 'å¦‚æœ', 'å› ä¸º',
                     'æ‰€ä»¥', 'ç„¶å', 'é‚£ä¹ˆ', 'ä¸€ä¸‹', 'ä¸€ç‚¹', 'ä¸€äº›', 'ä¸€ä¸ª', 'ä¸€ç§', 'ä¸€æ ·']
        
        words = text.split()
        keywords = []
        for word in words:
            if word not in stop_words and len(word) > 1:
                keywords.append(word)
        
        return keywords
    
    def suggest_conversation_topic(self, user_input: str) -> str:
        """å»ºè®®å¯¹è¯è¯é¢˜"""
        return self.db.suggest_topic(user_input)
    
    def get_recent_history(self, user_input: str, limit: int = 5) -> List[Dict]:
        """è·å–æœ€è¿‘å¯¹è¯å†å²"""
        return self.db.get_recent_conversations(user_input, limit)

# ä¼˜åŒ–é‡åŒ–é…ç½®å’Œæ˜¾å­˜ä½¿ç”¨
DEVICE = "cuda" 
QUANTIZE = 8  # ä»4bitæ”¹ä¸º8bité‡åŒ–ï¼Œå¹³è¡¡é€Ÿåº¦å’Œå†…å­˜

def init_model_and_tokenizer():
    """ä¼˜åŒ–æ¨¡å‹åŠ è½½ï¼Œæ·»åŠ è®°å¿†ç³»ç»Ÿ"""
    tokenizer = AutoTokenizer.from_pretrained(
        LOCAL_MODEL_PATH, 
        trust_remote_code=True,
        use_fast=True
    )
    
    model = AutoModel.from_pretrained(
        LOCAL_MODEL_PATH,
        trust_remote_code=True,
        low_cpu_mem_usage=True,
        torch_dtype=torch.float16,
    )
    
    if QUANTIZE > 0 and DEVICE == "cuda":
        model = model.quantize(QUANTIZE)
        print(f"âœ… æ¨¡å‹å·²åŠ è½½{QUANTIZE}bité‡åŒ–ç‰ˆæœ¬")
    
    model = model.to(DEVICE).eval()
    
    if DEVICE == "cuda":
        model = torch.compile(model)
        torch.cuda.empty_cache()
    
    # åˆå§‹åŒ–è®°å¿†ç³»ç»Ÿ
    memory_system = MemorySystem()
    
    return tokenizer, model, memory_system
def normal_chat(tokenizer, model):
    """æ™®é€šå¯¹è¯æ¨¡å¼ï¼ˆä¸€æ¬¡æ€§è¿”å›ç»“æœï¼Œæ— AIèº«ä»½è®¤çŸ¥ï¼‰"""
    # åˆå§‹åŒ–å¯¹è¯å†å²ï¼šä»…åŒ…å«è‡ªå®šä¹‰System Promptï¼Œæ— å…¶ä»–åˆå§‹ä¿¡æ¯
    history = []
    # å…³é”®ï¼šç»™ChatGLM3ä¼ å…¥è‡ªå®šä¹‰System Promptï¼ˆè¦†ç›–é»˜è®¤AIèº«ä»½ï¼‰
    # ChatGLM3çš„chatæ¥å£é€šè¿‡historyé—´æ¥ä¼ å…¥system prompt
    history.append({"role": "system", "content": CUSTOM_SYSTEM_PROMPT})
    
    print("\n===== miricleï¼ˆè¾“å…¥exité€€å‡ºï¼‰=====\n")  # æ”¹æ ‡é¢˜ï¼Œå»æ‰ChatGLM3æ ‡è¯†
    while True:
        # è·å–ç”¨æˆ·è¾“å…¥
        user_input = input("ğŸ‘¤ ä½ : ").strip()
        if user_input.lower() == "exit":
            print("ğŸ‘‹ å¯¹è¯ç»“æŸ")
            ##é‡Šæ”¾èµ„æº
            del tokenizer, model
            break
        if not user_input:
            print("âš ï¸ è¾“å…¥ä¸èƒ½ä¸ºç©ºï¼Œè¯·é‡æ–°è¾“å…¥ï¼")
            continue
        
        # è°ƒç”¨æ¨¡å‹ï¼ˆé€‚é…ChatGLM3çš„chatæ¥å£ï¼Œä¼ å…¥è‡ªå®šä¹‰systemï¼‰
        try:
            response, history = model.chat(
                tokenizer,
                user_input,
                history=history,
                top_p=1.0,
                temperature=1.0,
                system=CUSTOM_SYSTEM_PROMPT  # æ˜¾å¼ä¼ å…¥è‡ªå®šä¹‰systemï¼ŒåŒé‡ä¿éšœ
            )
            # è¿‡æ»¤å¯èƒ½æ¼å‡ºçš„AIèº«ä»½å…³é”®è¯ï¼ˆå…œåº•ï¼‰
            filter_words = ["AI", "åŠ©æ‰‹", "ChatGLM", "æ¨¡å‹", "è®­ç»ƒ", "å¼€å‘", "æ™ºè°±"]
            for word in filter_words:
                response = response.replace(word, "")
            # è¾“å‡ºå›å¤ï¼ˆå»æ‰ChatGLM3æ ‡è¯†ï¼‰
            print(f"miricle: {response}\n")
        except Exception as e:
            print(f"âŒ å¯¹è¯å‡ºé”™ï¼š{e}")

def stream_chat(tokenizer, model):
    """æµå¼å¯¹è¯æ¨¡å¼ï¼ˆé€å­—è¾“å‡ºï¼Œæ— AIèº«ä»½è®¤çŸ¥ï¼‰"""
    # åˆå§‹åŒ–å¯¹è¯å†å²ï¼šä»…åŒ…å«è‡ªå®šä¹‰System Promptï¼Œæ— å…¶ä»–åˆå§‹ä¿¡æ¯
    history = []
    history.append({"role": "system", "content": CUSTOM_SYSTEM_PROMPT})
    
    print("\n===== èŠå¤©ä¼™ä¼´ï¼ˆè¾“å…¥exité€€å‡ºï¼‰=====\n")  # æ”¹æ ‡é¢˜ï¼Œå»æ‰ChatGLM3æ ‡è¯†
    while True:
        user_input = input("ğŸ‘¤ ä½ : ").strip()
        if user_input.lower() == "exit":
            print("ğŸ‘‹ å¯¹è¯ç»“æŸ")
            break
        if not user_input:
            print("âš ï¸ è¾“å…¥ä¸èƒ½ä¸ºç©ºï¼Œè¯·é‡æ–°è¾“å…¥ï¼")
            continue
        
        # æµå¼è°ƒç”¨æ¨¡å‹ï¼ˆä¼ å…¥è‡ªå®šä¹‰systemï¼‰
        try:
            print("miricle: ", end="", flush=True)
            final_response = ""
            # é€å­—ç”Ÿæˆå›å¤
            for response, history, _ in model.stream_chat(
                tokenizer,
                user_input,
                history=history,
                top_p=1.0,
                temperature=1.0,
                system=CUSTOM_SYSTEM_PROMPT,  # æ˜¾å¼ä¼ å…¥è‡ªå®šä¹‰system
                past_key_values=None,
                return_past_key_values=True
            ):
                # è¿‡æ»¤AIèº«ä»½å…³é”®è¯
                filter_words = ["AI", "åŠ©æ‰‹", "ChatGLM", "æ¨¡å‹", "è®­ç»ƒ", "å¼€å‘", "æ™ºè°±"]
                for word in filter_words:
                    response = response.replace(word, "")
                # è¾“å‡ºæ–°å¢çš„å†…å®¹ï¼ˆé¿å…é‡å¤æ‰“å°ï¼‰
                new_content = response[len(final_response):]
                print(new_content, end="", flush=True)
                final_response = response
            print("\n")  # æ¢è¡Œåˆ†éš”
        except Exception as e:
            print(f"\nâŒ æµå¼å¯¹è¯å‡ºé”™ï¼š{e}")

if __name__ == "__main__":
    # åˆå§‹åŒ–æ¨¡å‹å’Œtokenizer
    tokenizer, model = init_model_and_tokenizer()
    # é€‰æ‹©å¯¹è¯æ¨¡å¼ï¼šé»˜è®¤æ™®é€šæ¨¡å¼ï¼Œæƒ³æµå¼å°±æŠŠFalseæ”¹True
    USE_STREAM = True
    if USE_STREAM:
        stream_chat(tokenizer, model)
    else:
        normal_chat(tokenizer, model)
# åœ¨ llm_zhipu_driver.py æœ«å°¾æ·»åŠ 

def create_stream_generator(tokenizer, model, query: str, history: list, memory_system=None):
    """
    å¸¦è®°å¿†çš„æµå¼ç”Ÿæˆå™¨
    """
    # å‡†å¤‡è®°å¿†ä¸Šä¸‹æ–‡
    memory_context = ""
    
    if memory_system:
        try:
            # è·å–è®°å¿†ä¸Šä¸‹æ–‡
            memory_context = memory_system.get_memory_context(query)
            print(f"ğŸ§  è®°å¿†ä¸Šä¸‹æ–‡:\n{memory_context}")  # è°ƒè¯•ä¿¡æ¯
        except Exception as e:
            print(f"âš ï¸ è®°å¿†ç³»ç»Ÿé”™è¯¯: {e}")
            memory_context = ""
    
    # æ„å»ºåŠ¨æ€æç¤ºè¯
    dynamic_prompt = CUSTOM_SYSTEM_PROMPT.format(
        memory_context=memory_context
    )
    
    # ç¡®ä¿historyä»¥è‡ªå®šä¹‰system promptå¼€å¤´
    if not history or history[0].get("role") != "system":
        history = [{"role": "system", "content": dynamic_prompt}]
    else:
        # æ›´æ–°system prompt
        history[0]["content"] = dynamic_prompt
    
    # ä½¿ç”¨æ¨¡å‹çš„stream_chatæ–¹æ³•
    full_response = ""
    for response, new_history, _ in model.stream_chat(
        tokenizer=tokenizer,
        query=query,
        history=history,
        top_p=0.9,
        temperature=0.8,
        system=dynamic_prompt,
        past_key_values=None,
        return_past_key_values=True
    ):
        # è¿‡æ»¤AIèº«ä»½å…³é”®è¯
        filter_words = ["AI", "åŠ©æ‰‹", "ChatGLM", "æ¨¡å‹", "è®­ç»ƒ", "å¼€å‘", "æ™ºè°±", "äººå·¥æ™ºèƒ½"]
        filtered_response = response
        for word in filter_words:
            filtered_response = filtered_response.replace(word, "")
        
        # æå–æ–°å¢çš„å†…å®¹
        if len(filtered_response) > len(full_response):
            new_content = filtered_response[len(full_response):]
            full_response = filtered_response
            
            yield new_content, new_history, full_response
    
    # æœ€åyieldå®Œæ•´å›å¤
    yield "", new_history, full_response
# åœ¨ llm_zhipu_driver.py ä¸­æ·»åŠ æµå¼ç”Ÿæˆå™¨
def stream_chat_with_memory(tokenizer, model, user_input, history=None, memory_system=None, temperature=0.8):
    """
    å¸¦è®°å¿†çš„æµå¼å¯¹è¯ç”Ÿæˆå™¨
    è¿”å›ï¼š(chunk, is_final, full_response)
    """
    # å‡†å¤‡è®°å¿†ä¸Šä¸‹æ–‡
    memory_context = ""
    if memory_system:
        try:
            memory_context = memory_system.get_memory_context(user_input)
            if memory_context and "ï¼ˆæš‚æ— è®°å¿†ï¼‰" not in memory_context:
                print(f"ğŸ§  ä½¿ç”¨è®°å¿†: {memory_context}")
        except Exception as e:
            print(f"âš ï¸ è®°å¿†ç³»ç»Ÿé”™è¯¯: {e}")
            memory_context = ""
    
    # æ„å»ºåŠ¨æ€æç¤ºè¯
    dynamic_prompt = CUSTOM_SYSTEM_PROMPT.format(
        memory_context=memory_context
    )
    
    # å‡†å¤‡å¯¹è¯å†å²
    if not history or history[0].get("role") != "system":
        history = [{"role": "system", "content": dynamic_prompt}]
    else:
        history[0]["content"] = dynamic_prompt
    
    # æ·»åŠ ç”¨æˆ·è¾“å…¥
    history.append({"role": "user", "content": user_input})
    
    print(f"ğŸ§  LLMå¼€å§‹ç”Ÿæˆ...")
    
    # ä½¿ç”¨æ¨¡å‹çš„stream_chatæ–¹æ³•è·å–æµå¼å“åº”
    full_response = ""
    chunk_count = 0
    
    for response, new_history, _ in model.stream_chat(
        tokenizer=tokenizer,
        query=user_input,
        history=history,
        top_p=0.9,
        temperature=temperature,
        system=dynamic_prompt,
        past_key_values=None,
        return_past_key_values=True
    ):
        # è¿‡æ»¤AIèº«ä»½å…³é”®è¯
        filter_words = ["AI", "åŠ©æ‰‹", "ChatGLM", "æ¨¡å‹", "è®­ç»ƒ", "å¼€å‘", "æ™ºè°±", "äººå·¥æ™ºèƒ½", "è¯­è¨€æ¨¡å‹"]
        filtered_response = response
        for word in filter_words:
            filtered_response = filtered_response.replace(word, "")
        
        # æå–æ–°å¢çš„å†…å®¹
        if len(filtered_response) > len(full_response):
            new_content = filtered_response[len(full_response):]
            full_response = filtered_response
            
            if new_content:
                chunk_count += 1
                # print(f"ğŸ“ LLMç”Ÿæˆç¬¬{chunk_count}ä¸ªåˆ†ç‰‡: {new_content[:30]}...")
                yield new_content, False, full_response
    
    # æœ€ç»ˆyieldå®Œæ•´å›å¤å’Œç»“æŸæ ‡è®°
    yield "", True, full_response
    
    print(f"âœ… LLMç”Ÿæˆå®Œæˆï¼Œå…±{chunk_count}ä¸ªåˆ†ç‰‡")