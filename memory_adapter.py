"""
è®°å¿†é€‚é…å™¨ï¼šå°†å¢å¼ºè®°å¿†æ¨¡å—é›†æˆåˆ°ç°æœ‰ç³»ç»Ÿ
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from enhanced_memory import EnhancedMemoryLLM
from typing import Optional

class MemoryAdapter:
    """è®°å¿†é€‚é…å™¨ï¼šæ¡¥æ¥ç°æœ‰ç³»ç»Ÿå’Œå¢å¼ºè®°å¿†æ¨¡å—"""
    
    def __init__(self, base_model, tokenizer):
        """åˆå§‹åŒ–é€‚é…å™¨"""
        print("ğŸš€ åˆå§‹åŒ–å¢å¼ºè®°å¿†é€‚é…å™¨...")
        
        # åˆ›å»ºå¢å¼ºè®°å¿†LLM
        self.enhanced_llm = EnhancedMemoryLLM(base_model, tokenizer)
        
        # çŠ¶æ€è·Ÿè¸ª
        self.conversation_count = 0
        self.memory_hits = 0
        self.last_query = ""
        
        print("âœ… å¢å¼ºè®°å¿†é€‚é…å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def process_query(self, query: str, use_memory: bool = True, 
                     force_memory: bool = False, **kwargs) -> str:
        """å¤„ç†æŸ¥è¯¢"""
        self.conversation_count += 1
        self.last_query = query
        
        # æ£€æŸ¥æ˜¯å¦åº”è¯¥å¼ºåˆ¶ä½¿ç”¨è®°å¿†
        should_force = force_memory or self._should_force_memory(query)
        
        if should_force:
            response = self.enhanced_llm.force_memory_use(query)
            self.memory_hits += 1
        elif use_memory:
            response = self.enhanced_llm.chat(query, **kwargs)
        else:
            response = self.enhanced_llm.chat(query, use_memory=False, **kwargs)
        
        # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨äº†è®°å¿†
        memory_context = self.enhanced_llm.memory_system.get_memory_context(query)
        if "ï¼ˆæš‚æ— è®°å¿†ï¼‰" not in memory_context:
            self.memory_hits += 1
        
        return response
    
    def _should_force_memory(self, query: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥å¼ºåˆ¶ä½¿ç”¨è®°å¿†"""
        # æ£€æŸ¥æ˜¯å¦æ˜¯é‡å¤é—®é¢˜
        if query == self.last_query:
            return True
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«ç‰¹å®šå…³é”®è¯
        force_keywords = ['ä¹‹å‰è¯´è¿‡', 'åˆšæ‰è¯´', 'è®°å¾—å—', 'è¿˜è®°å¾—å—', 'è¯´è¿‡']
        for keyword in force_keywords:
            if keyword in query:
                return True
        
        return False
    
    def get_stats(self) -> dict:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        return {
            'conversation_count': self.conversation_count,
            'memory_hits': self.memory_hits,
            'memory_hit_rate': self.memory_hits / max(self.conversation_count, 1),
            'short_term_memory_size': len(self.enhanced_llm.memory_system.short_term_memory)
        }
    
    def clear_memory(self):
        """æ¸…ç©ºè®°å¿†"""
        self.enhanced_llm.clear_memory()
        self.conversation_count = 0
        self.memory_hits = 0
        self.last_query = ""
        print("ğŸ§¹ è®°å¿†å·²æ¸…ç©º")
    
    def export_memory(self, filepath: str = None):
        """å¯¼å‡ºè®°å¿†"""
        return self.enhanced_llm.export_memory(filepath)
    
    def get_memory_context(self, query: str) -> str:
        """è·å–è®°å¿†ä¸Šä¸‹æ–‡"""
        return self.enhanced_llm.memory_system.get_memory_context(query)
    
    def get_facts_by_entity(self, entity: str) -> list:
        """è·å–å®ä½“çš„æ‰€æœ‰äº‹å®"""
        return self.enhanced_llm.memory_system.get_facts_by_entity(entity)
    
    def manual_add_fact(self, fact_text: str, fact_type: str = "manual"):
        """æ‰‹åŠ¨æ·»åŠ äº‹å®"""
        self.enhanced_llm.memory_system.process_conversation(
            f"æ‰‹åŠ¨æ·»åŠ äº‹å®: {fact_text}",
            f"å·²ç¡®è®¤äº‹å®: {fact_text}"
        )
        print(f"ğŸ“ æ‰‹åŠ¨æ·»åŠ äº‹å®: {fact_text}")
def process_query_stream(self, user_input: str, use_memory: bool = True, 
                         force_memory: bool = False, temperature: float = 0.8):
    """æµå¼å¤„ç†æŸ¥è¯¢ï¼Œè¿”å›ç”Ÿæˆå™¨"""
    # å‡†å¤‡è®°å¿†ä¸Šä¸‹æ–‡
    memory_context = ""
    memory_hit = False
    
    if use_memory:
        try:
            memory_context = self.memory_system.get_memory_context(user_input)
            if memory_context and "ï¼ˆæš‚æ— è®°å¿†ï¼‰" not in memory_context:
                memory_hit = True
        except Exception as e:
            print(f"âš ï¸ è®°å¿†ç³»ç»Ÿé”™è¯¯: {e}")
            memory_context = ""
    
    # æ„å»ºåŠ¨æ€æç¤ºè¯
    dynamic_prompt = self.system_prompt.format(
        memory_context=memory_context
    )
    
    # å‡†å¤‡å¯¹è¯å†å²
    if not self.history or self.history[0].get("role") != "system":
        self.history = [{"role": "system", "content": dynamic_prompt}]
    else:
        self.history[0]["content"] = dynamic_prompt
    
    # ä½¿ç”¨æ¨¡å‹çš„stream_chatæ–¹æ³•
    full_response = ""
    for response, new_history, _ in self.model.stream_chat(
        tokenizer=self.tokenizer,
        query=user_input,
        history=self.history,
        top_p=0.9,
        temperature=temperature,
        system=dynamic_prompt,
        past_key_values=None,
        return_past_key_values=True
    ):
        # è¿‡æ»¤AIèº«ä»½å…³é”®è¯
        filter_words = ["AI", "åŠ©æ‰‹", "ChatGLM", "æ¨¡å‹", "è®­ç»ƒ", "å¼€å‘", "æ™ºè°±", "äººå·¥æ™ºèƒ½"]
        filtered_response = response
        for word in filter_words:
            filtered_response = filtered_response.replace(word, "")
        
        # æå–æ–°å¢çš„å†…å®¹
        if len(filtered_response) > len(full_response):
            new_content = filtered_response[len(full_response):]
            full_response = filtered_response
            
            yield new_content, False
    
    # æœ€ç»ˆæ ‡è®°
    yield "", True
    
    # æ›´æ–°å†å²
    self.history = [{"role": "system", "content": dynamic_prompt},
                    {"role": "user", "content": user_input},
                    {"role": "assistant", "content": full_response}]
    
    # æ›´æ–°ç»Ÿè®¡
    self.conversation_count += 1
    if memory_hit:
        self.memory_hits += 1
    
    # å­˜å‚¨åˆ°è®°å¿†ç³»ç»Ÿ
    if use_memory and self.memory_system:
        self.memory_system.analyze_and_store(user_input, full_response)