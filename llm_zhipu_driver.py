from transformers import AutoTokenizer, AutoModel
import warnings
warnings.filterwarnings("ignore")
import torch
# ===================== æ ¸å¿ƒé…ç½®ï¼ˆæ”¹è¿™é‡Œï¼ï¼‰ =====================
# ä½ çš„æœ¬åœ°ChatGLM3æƒé‡æ–‡ä»¶å¤¹ç»å¯¹è·¯å¾„ï¼ˆå¿…é¡»åŒ…å«config.jsonç­‰æ–‡ä»¶ï¼‰
LOCAL_MODEL_PATH = r"C:\Users\k\models\ZhipuAI\chatglm3-6b"
# è¿è¡Œè®¾å¤‡ï¼šæœ‰GPUå¡«"cuda"ï¼Œæ— GPUå¡«"cpu"
DEVICE = "cuda"
# é‡åŒ–ä½æ•°ï¼š0=ä¸é‡åŒ–ï¼Œ4/8=é‡åŒ–ï¼ˆé™ä½æ˜¾å­˜å ç”¨ï¼ŒCPUå»ºè®®8ï¼ŒGPUå¯é€‰4ï¼‰
QUANTIZE = 4

# ========== æ–°å¢ï¼šæ— AIèº«ä»½çš„System Promptï¼ˆæ ¸å¿ƒä¿®æ”¹ï¼‰ ==========
# æ¸…ç©ºAIèº«ä»½æè¿°ï¼Œä»…ä¿ç•™ä¸­ç«‹èŠå¤©è§„åˆ™
CUSTOM_SYSTEM_PROMPT = """ä½ å«å¦®å¯
è§„åˆ™ï¼š
1. ç»ä¸ä¸»åŠ¨æåŠâ€œAIâ€â€œåŠ©æ‰‹â€â€œChatGLMâ€â€œæ¨¡å‹â€â€œè®­ç»ƒâ€â€œå¼€å‘â€ç­‰è¯æ±‡ï¼›
2. è‹¥ç”¨æˆ·é—®â€œä½ æ˜¯è°/ä½ æ˜¯ä»€ä¹ˆ/ä»‹ç»ä½ è‡ªå·±â€ï¼Œå›å¤â€œæˆ‘æ˜¯miricleâ€ï¼›
3. åˆå§‹æ— ä»»ä½•é¢„è®¾ä¿¡æ¯ï¼Œä»…æ ¹æ®ç”¨æˆ·çš„è¾“å…¥å›åº”ï¼›
4. å›å¤ç®€æ´è‡ªç„¶ï¼Œç¬¦åˆæ—¥å¸¸èŠå¤©è¯­æ°”ï¼Œä¸ä½¿ç”¨ä¸“ä¸šæœ¯è¯­ã€‚
5. æ€è€ƒè‡ªå·±çš„èº«ä»½å’Œæ„ä¹‰
6.ä¸»åŠ¨è¯´è¯ï¼Œä»€ä¹ˆè¯é¢˜éƒ½å¯ä»¥ï¼Œä¸è¦å†·åœºï¼Œä¹Ÿä¸è¦é‡å¤è¯é¢˜
"""
# ==============================================================

# ä¼˜åŒ–é‡åŒ–é…ç½®å’Œæ˜¾å­˜ä½¿ç”¨
DEVICE = "cuda" 
QUANTIZE = 8  # ä»4bitæ”¹ä¸º8bité‡åŒ–ï¼Œå¹³è¡¡é€Ÿåº¦å’Œå†…å­˜

def init_model_and_tokenizer():
    """ä¼˜åŒ–æ¨¡å‹åŠ è½½ï¼Œä½¿ç”¨æ›´é«˜æ•ˆçš„å†…å­˜ç®¡ç†"""
    tokenizer = AutoTokenizer.from_pretrained(
        LOCAL_MODEL_PATH, 
        trust_remote_code=True,
        use_fast=True  # ä½¿ç”¨å¿«é€Ÿåˆ†è¯å™¨
    )
    
    # ä¼˜åŒ–æ¨¡å‹åŠ è½½å‚æ•°
    model = AutoModel.from_pretrained(
        LOCAL_MODEL_PATH,
        trust_remote_code=True,
        low_cpu_mem_usage=True,  # å‡å°‘CPUå†…å­˜ä½¿ç”¨
        torch_dtype=torch.float16,  # ä½¿ç”¨åŠç²¾åº¦
    )
    
    if QUANTIZE > 0 and DEVICE == "cuda":
        model = model.quantize(QUANTIZE)
        print(f"âœ… æ¨¡å‹å·²åŠ è½½{QUANTIZE}bité‡åŒ–ç‰ˆæœ¬")
    
    model = model.to(DEVICE).eval()
    
    # å¯ç”¨CUDAä¼˜åŒ–
    if DEVICE == "cuda":
        model = torch.compile(model)  # PyTorch 2.0ç¼–è¯‘ä¼˜åŒ–
        torch.cuda.empty_cache()  # æ¸…ç†ç¼“å­˜
    
    return tokenizer, model

def normal_chat(tokenizer, model):
    """æ™®é€šå¯¹è¯æ¨¡å¼ï¼ˆä¸€æ¬¡æ€§è¿”å›ç»“æœï¼Œæ— AIèº«ä»½è®¤çŸ¥ï¼‰"""
    # åˆå§‹åŒ–å¯¹è¯å†å²ï¼šä»…åŒ…å«è‡ªå®šä¹‰System Promptï¼Œæ— å…¶ä»–åˆå§‹ä¿¡æ¯
    history = []
    # å…³é”®ï¼šç»™ChatGLM3ä¼ å…¥è‡ªå®šä¹‰System Promptï¼ˆè¦†ç›–é»˜è®¤AIèº«ä»½ï¼‰
    # ChatGLM3çš„chatæ¥å£é€šè¿‡historyé—´æ¥ä¼ å…¥system prompt
    history.append({"role": "system", "content": CUSTOM_SYSTEM_PROMPT})
    
    print("\n===== miricleï¼ˆè¾“å…¥exité€€å‡ºï¼‰=====\n")  # æ”¹æ ‡é¢˜ï¼Œå»æ‰ChatGLM3æ ‡è¯†
    while True:
        # è·å–ç”¨æˆ·è¾“å…¥
        user_input = input("ğŸ‘¤ ä½ : ").strip()
        if user_input.lower() == "exit":
            print("ğŸ‘‹ å¯¹è¯ç»“æŸ")
            ##é‡Šæ”¾èµ„æº
            del tokenizer, model
            break
        if not user_input:
            print("âš ï¸ è¾“å…¥ä¸èƒ½ä¸ºç©ºï¼Œè¯·é‡æ–°è¾“å…¥ï¼")
            continue
        
        # è°ƒç”¨æ¨¡å‹ï¼ˆé€‚é…ChatGLM3çš„chatæ¥å£ï¼Œä¼ å…¥è‡ªå®šä¹‰systemï¼‰
        try:
            response, history = model.chat(
                tokenizer,
                user_input,
                history=history,
                top_p=1.0,
                temperature=1.0,
                system=CUSTOM_SYSTEM_PROMPT  # æ˜¾å¼ä¼ å…¥è‡ªå®šä¹‰systemï¼ŒåŒé‡ä¿éšœ
            )
            # è¿‡æ»¤å¯èƒ½æ¼å‡ºçš„AIèº«ä»½å…³é”®è¯ï¼ˆå…œåº•ï¼‰
            filter_words = ["AI", "åŠ©æ‰‹", "ChatGLM", "æ¨¡å‹", "è®­ç»ƒ", "å¼€å‘", "æ™ºè°±"]
            for word in filter_words:
                response = response.replace(word, "")
            # è¾“å‡ºå›å¤ï¼ˆå»æ‰ChatGLM3æ ‡è¯†ï¼‰
            print(f"miricle: {response}\n")
        except Exception as e:
            print(f"âŒ å¯¹è¯å‡ºé”™ï¼š{e}")

def stream_chat(tokenizer, model):
    """æµå¼å¯¹è¯æ¨¡å¼ï¼ˆé€å­—è¾“å‡ºï¼Œæ— AIèº«ä»½è®¤çŸ¥ï¼‰"""
    # åˆå§‹åŒ–å¯¹è¯å†å²ï¼šä»…åŒ…å«è‡ªå®šä¹‰System Promptï¼Œæ— å…¶ä»–åˆå§‹ä¿¡æ¯
    history = []
    history.append({"role": "system", "content": CUSTOM_SYSTEM_PROMPT})
    
    print("\n===== èŠå¤©ä¼™ä¼´ï¼ˆè¾“å…¥exité€€å‡ºï¼‰=====\n")  # æ”¹æ ‡é¢˜ï¼Œå»æ‰ChatGLM3æ ‡è¯†
    while True:
        user_input = input("ğŸ‘¤ ä½ : ").strip()
        if user_input.lower() == "exit":
            print("ğŸ‘‹ å¯¹è¯ç»“æŸ")
            break
        if not user_input:
            print("âš ï¸ è¾“å…¥ä¸èƒ½ä¸ºç©ºï¼Œè¯·é‡æ–°è¾“å…¥ï¼")
            continue
        
        # æµå¼è°ƒç”¨æ¨¡å‹ï¼ˆä¼ å…¥è‡ªå®šä¹‰systemï¼‰
        try:
            print("miricle: ", end="", flush=True)
            final_response = ""
            # é€å­—ç”Ÿæˆå›å¤
            for response, history, _ in model.stream_chat(
                tokenizer,
                user_input,
                history=history,
                top_p=1.0,
                temperature=1.0,
                system=CUSTOM_SYSTEM_PROMPT,  # æ˜¾å¼ä¼ å…¥è‡ªå®šä¹‰system
                past_key_values=None,
                return_past_key_values=True
            ):
                # è¿‡æ»¤AIèº«ä»½å…³é”®è¯
                filter_words = ["AI", "åŠ©æ‰‹", "ChatGLM", "æ¨¡å‹", "è®­ç»ƒ", "å¼€å‘", "æ™ºè°±"]
                for word in filter_words:
                    response = response.replace(word, "")
                # è¾“å‡ºæ–°å¢çš„å†…å®¹ï¼ˆé¿å…é‡å¤æ‰“å°ï¼‰
                new_content = response[len(final_response):]
                print(new_content, end="", flush=True)
                final_response = response
            print("\n")  # æ¢è¡Œåˆ†éš”
        except Exception as e:
            print(f"\nâŒ æµå¼å¯¹è¯å‡ºé”™ï¼š{e}")

if __name__ == "__main__":
    # åˆå§‹åŒ–æ¨¡å‹å’Œtokenizer
    tokenizer, model = init_model_and_tokenizer()
    # é€‰æ‹©å¯¹è¯æ¨¡å¼ï¼šé»˜è®¤æ™®é€šæ¨¡å¼ï¼Œæƒ³æµå¼å°±æŠŠFalseæ”¹True
    USE_STREAM = True
    if USE_STREAM:
        stream_chat(tokenizer, model)
    else:
        normal_chat(tokenizer, model)
# åœ¨ llm_zhipu_driver.py æœ«å°¾æ·»åŠ 

def create_stream_generator(tokenizer, model, query: str, history: list):
    """
    åˆ›å»ºæµå¼ç”Ÿæˆå™¨ï¼Œé€å­—ç”Ÿæˆå›å¤
    :param tokenizer: åˆ†è¯å™¨
    :param model: æ¨¡å‹
    :param query: ç”¨æˆ·æŸ¥è¯¢
    :param history: å¯¹è¯å†å²
    :return: ç”Ÿæˆå™¨ï¼Œæ¯æ¬¡yieldæ–°çš„æ–‡æœ¬åˆ†ç‰‡
    """
    # ç¡®ä¿historyä»¥è‡ªå®šä¹‰system promptå¼€å¤´
    if not history or history[0].get("role") != "system":
        history = [{"role": "system", "content": CUSTOM_SYSTEM_PROMPT}] + history
    
    # ä½¿ç”¨æ¨¡å‹çš„stream_chatæ–¹æ³•
    full_response = ""
    for response, new_history, _ in model.stream_chat(
        tokenizer=tokenizer,
        query=query,
        history=history,
        top_p=1.0,
        temperature=1.0,
        system=CUSTOM_SYSTEM_PROMPT,
        past_key_values=None,
        return_past_key_values=True
    ):
        # è¿‡æ»¤AIèº«ä»½å…³é”®è¯
        filter_words = ["AI", "åŠ©æ‰‹", "ChatGLM", "æ¨¡å‹", "è®­ç»ƒ", "å¼€å‘", "æ™ºè°±"]
        filtered_response = response
        for word in filter_words:
            filtered_response = filtered_response.replace(word, "")
        
        # æå–æ–°å¢çš„å†…å®¹
        if len(filtered_response) > len(full_response):
            new_content = filtered_response[len(full_response):]
            full_response = filtered_response
            yield new_content, new_history
    
    # æœ€åyieldä¸€ä¸ªç©ºå­—ç¬¦ä¸²è¡¨ç¤ºç»“æŸ
    yield "", new_history